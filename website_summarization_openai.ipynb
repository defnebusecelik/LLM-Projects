{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ae8325-6ffd-4ad6-938d-4a832fa09222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.6.1-cp312-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
      "   ---------------------------------------- 0.0/383.7 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 41.0/383.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ------ -------------------------------- 61.4/383.7 kB 812.7 kB/s eta 0:00:01\n",
      "   ----------- -------------------------- 112.6/383.7 kB 930.9 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 143.4/383.7 kB 847.9 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 235.5/383.7 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 337.9/383.7 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 383.7/383.7 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 71.7/76.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 76.4/76.4 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.0 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 71.7/78.0 kB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 78.0/78.0 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading jiter-0.6.1-cp312-none-win_amd64.whl (198 kB)\n",
      "   ---------------------------------------- 0.0/199.0 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 92.2/199.0 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  194.6/199.0 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 199.0/199.0 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: jiter, h11, httpcore, httpx, openai\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 openai-1.51.2\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b87cadb-d513-4303-baee-a37b6f938e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'openai')\n",
    "openai = OpenAI()\n",
    "\n",
    "# openai = OpenAI(api_key=\"your-key-here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e793b2-6775-426a-a139-4848291d0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "class Website:\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef960cf-6dc2-4cda-afb3-b38be12f4c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Temelleri. LangChain Nedir? | by Defne Buse Çelik | Global AI Hub | Oct, 2024 | Medium\n",
      "Open in app\n",
      "Sign up\n",
      "Sign in\n",
      "Write\n",
      "Sign up\n",
      "Sign in\n",
      "LangChain Temelleri\n",
      "Defne Buse Çelik\n",
      "·\n",
      "Follow\n",
      "Published in\n",
      "Global AI Hub\n",
      "·\n",
      "10 min read\n",
      "·\n",
      "Just now\n",
      "--\n",
      "Share\n",
      "LangChain Nedir?\n",
      "LangChain, doğal dil işleme projelerinin esnekliğini ve gücünü artıran yenilikçi bir çerçevedir. Büyük dil modelleri (LLM) ile derin etkileşim sağlayarak metin oluşturma, sorgulama, analiz ve veri kaynaklarıyla entegrasyon gibi çok yönlü görevler için optimize edilmiştir.\n",
      "LangChain, süreçleri modüler bir yapıya dönüştürmektedir ve geliştiricilerin adım adım zincirler oluşturmalarını sağlamaktadır. Bu sayede kullanıcıların karmaşık dil işleme görevlerini tek bir çerçeve içinde ve organize bir halde ölçeklenebilir çözümleri kolaylaştırmalarını sağlar.\n",
      "LangChain, LLM uygulamalarının gelişimini hızlandırmak için tasarlanmıştır. LangChain, yazılım geliştiricilerinin harici bileşenler ve veri kaynaklarıyla LLM uygulamalarını entegre ederek hızlı bir şekilde oluşturulmalarını sağlar. Geliştiriciler LangChain uygulamalarını, Python veya JavaScript / TypeScript kullanarak oluşturabilir.\n",
      "LangChain Bileşenleri\n",
      "LangChain birden fazla büyük dil modelini destekler ve modeli tek bir parametre değişikliği ile değiştirmenizi sağlar. Hızlı bir şekilde birden fazla dil modelini test edebilir ve farklı kullanım durumları için farklı modeller kullanabilirsiniz. LangChain’in esnekliği, farklı LLM sağlayıcılarını ve modellerini minimum kod değişiklikleriyle test etmenizi sağlamasıdır.\n",
      "LangChain’deki “chain”, bir işleme hattı (pipeline) oluşturarak uygulanacak eylemleri bir araya getirme fikrini oluşturmaktadır. Her eylem veya zincir, genel hedefi tamamlamak için işleme hattında gerekli bir adımdır. RAG modeli, kullanıcının bir soru göndermesiyle başlar. Metinden bir gömme modeli oluşturulur, vektör veritabanında soru hakkında daha fazla bağlam kazanmak için bir arama yapılır, hem orijinal soru hem de geri kazanımın bağlamı kullanılarak bir istem(prompt) oluşturulur, büyük dil modeline gönderilir.\n",
      "RAG modelinde tüm adımların başarılı bir şekilde gerçekleşmesi gerekmektedir. LangChain, belirli bir yapılandırma ile belirli bir şekilde adımları bir araya getirmek için zincir yapısı sunmaktadır. Tüm LangChain kütüphaneleri, adımları uygulamayı ve güçlü işlem hatları oluşturmayı kolaylaştırmak için zincir yapısını takip etmektedir.\n",
      "Nasıl Çalışmaktadır?\n",
      "LangChain uygulamaları, kullanıcılar ile büyük dil modelleri arasında köprü oluşturarak zincirler aracılığıyla ileri geri iletişim kurmayı sağlamaktadır.\n",
      "Bir LangChain uygulamasının temel bileşenleri şunlardır:\n",
      "Model Etkileşimi (Model I/O)\n",
      ": Dil modeliyle etkileşimi yöneten, girişleri beslemek ve çıktıları çıkarmak gibi görevleri denetleyen bileşenler.\n",
      "Verilerin Bağlantısı ve Bilgiye Erişim (Retrieval):\n",
      "Bilgiye erişim bileşenleri, verimli sorgular ve erişim işlemlerine olanak tanıyan verilere erişebilir, bdönüştürebilir ve depolayabilir. İndeksler, dil sorgularını işlerken ilgili verilerin verimli bir şekilde alınmasını sağlayan, bilgileri yapılandırılmış bir şekilde düzenleyen ve depolayan veritabanları olarak hizmet eder. Bilgiye erişim öğeleri, bu harici bağlamın kullanımına yardımcı olurken bir dil modeli tarafından bir yanıtın oluşturulmasına Almayla Artırılmış Üretim (RAG) denir.\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.document_loaders import TextLoader\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.text_splitter import CharacterTextSplitter\n",
      "from langchain.vectorstores import Chroma\n",
      "#Document loader\n",
      "loader = TextLoader(\"defnebusecelik.txt\")\n",
      "documents = loader.load()\n",
      "#Document Transformer\n",
      "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
      "texts = text_splitter.split_documents(documents)\n",
      "#Embedding model\n",
      "embeddings = OpenAIEmbeddings()\n",
      "#Vector DB to store embeddings\n",
      "docsearch = Chroma.from_documents(texts, embeddings)\n",
      "#Retriever being used using parameter retriever\n",
      "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n",
      "query = \"What this document about? Summarize it in a paragraph\"\n",
      "qa.run(query)\n",
      "RAG\n",
      "Zincirler (Chains) :\n",
      "Zincirler, bir isteme(prompt) dayalı bir talimatı yerine getirmenin en iyi yolunu belirleyen yeniden kullanılabilir bileşenlerdir. Bir sonraki dil modeli besleme çıkışıyla birden fazla dil modeli çağrılarını bir araya getirmenizi sağlar. Bu sayede çeşitli LLM işlevleri arasında karmaşık iş akışları ve bilgi çıkarmak yer alır.\n",
      "# Import LLMChain and define chain with language model and prompt as arguments.\n",
      "from langchain.chains import LLMChain\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "# Run the chain only specifying the input variable.\n",
      "print(chain.run(\"large language model\"))\n",
      "İstem(Prompt) Şablonları\n",
      ": LangChain, farklı dil modelleriyle nasıl etkileşime girdiğinizi standartlaştırmak için istem(prompt) şablonları kullanır. Bu şablonlar, temelde yer alan büyük dil modelinden bağımsız olarak istemlerin tutarlı biçimlendirilmesini ve ifade edilmesini sağlar.\n",
      "HumanMessage : Kullanıcı tarafından verilen istem\n",
      "AIMessage : LLM tarafından verilen yanıt\n",
      "SystemMessage: Sohbet modeline rolü hakkında bilgi aktarabileceğimiz bağlam\n",
      "from langchain.prompts import ChatPromptTemplate\n",
      "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "template = ChatPromptTemplate.from_messages(\n",
      "[\n",
      "SystemMessage(\n",
      "content=(\"You are a python coder AI that helps user with bugs\n",
      "and writing programs\")),\n",
      "HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
      "]\n",
      ")\n",
      "llm = ChatOpenAI()\n",
      "llm(template.format_messages(text='Check whether a number is prime or not'))\n",
      "Ajanlar:\n",
      "Ajanlar, büyük dil modellerine ve diğer araçlara yönelik komutları düzenler ve belirli görevleri yerine getirmelerini veya belirlenmiş sorunları çözmelerini sağlar.\n",
      "# Import Python REPL tool and instantiate Python agent\n",
      "from langchain.agents.agent_toolkits import create_python_agent\n",
      "from langchain.tools.python.tool import PythonREPLTool\n",
      "from langchain.python import PythonREPL\n",
      "from langchain.llms.openai import OpenAI\n",
      "agent_executor = create_python_agent(\n",
      "llm=OpenAI(temperature=0.6, max_tokens=1000),\n",
      "tool=PythonREPLTool(),\n",
      "verbose=True\n",
      ")\n",
      "agent_executor.run(\"What is the solution of this problem : A triangular array of squares has one square in the first row, two in the second, and in general, k squares in the kth row for 1 \\leq k \\leq 11.$ With the exception of the bottom row, each square rests on two squares in the row immediately below (illustrated in given diagram). In each square of the eleventh row, a 0$ or a $1$ is placed. Numbers are then placed into the other squares, with the entry for each square being the sum of the entries in the two squares below it. For how many initial distributions of $0$'s and $1$'s in the bottom row is the number in the top square a multiple of $3$??\")\n",
      "Bellek:\n",
      "Uygulamaların bağlamı korumasını sağlamak için kullanılır, örneğin bir konuşmadaki önceki mesajları hatırlamak. Birçok farklı bellek türü vardır. Hepsi farklı durumlarda faydalıdır ve farklı parametrelere ve dönüş türlerine sahiptir. Langchain’in Konuşma Arabellek Belleği, konuşma geçmişini depolayan temel bir bellek arabelleğidir. Sohbet belleğinden mesajların bir listesini döndüren bir arabellek özelliğine sahiptir. Bu bellek türü, bir tartışmanın en son geçmişini depolamak ve almak için önemlidir.\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "llm = OpenAI(temperature=0)\n",
      "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
      "Previous conversation:\n",
      "{chat_history}\n",
      "New human question: {question}\n",
      "Response:\"\"\"\n",
      "prompt = PromptTemplate.from_template(template)\n",
      "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "conversation = LLMChain(\n",
      "llm=llm,\n",
      "prompt=prompt,\n",
      "verbose=True,\n",
      "memory=memory\n",
      ")\n",
      "LangChain Mimarisi\n",
      "LangChain’in mimarisi, geliştiricilerin özel çözümler oluşturmak için bileşenleri karıştırıp eşleştirmelerine olanak tanıyan modüler ve genişletilebilir şekilde tasarlanmıştır.\n",
      "Langchain Mimarisi\n",
      "Bir modelin örneklenmesi— Codex, GPT-4 vb. için bir Python örneği oluşturun.\n",
      "Mantığın tanımlanması — Python’da uygulama mantığı ve iş akışları yazın.\n",
      "Zincirleme yürütme — Python kodunuzdan dil modelini çağırın.\n",
      "Sonuçları önbelleğe alma — Yeniden hesaplamayı önlemek için model çıktılarını önbelleğe alın.\n",
      "Önbelleklerin paylaşılması — Verimlilik için önbellek dağıtın.\n",
      "Modelleri karşılaştırma— Farklı modelleri karşılaştırın\n",
      "Ölçekte çalıştırma— Sunucusuz platformlardan yararlanın\n",
      "Temel mimari öğeler şunları içermektedir:\n",
      "LangChain:\n",
      "LangChain, geliştiricilerin karmaşık görevleri yönetilebilir adımlara bölerek kolaylaştıran zincirler halinde birden fazla bileşeni birleştirmelerine olanak tanır. Bu modülerlik, hata ayıklamayı ve bakımı basitleştirir.\n",
      "LangGraph:\n",
      "LangGraph,büyük dil modellerini kullanarak durum bilgisi olan, çok aktörlü uygulamaların oluşturulmasını sağlar. Adımları bir grafikteki kenarlar ve düğümler olarak modeller ve uygulama akışının görsel bir temsilini sağlar.\n",
      "LangServe:\n",
      "LangServe, geliştiricilerin LangChain uygulamalarını REST API’leri olarak dağıtmalarına olanak tanır ve bunları mevcut sistemlere ve iş akışlarına entegre etmeyi kolaylaştırır.\n",
      "LangSmith:\n",
      "LangSmith, büyük dil modeli uygulamalarının hata ayıklama, test etme ve izleme araçlarını sağlayarak geliştiricilerin çözümlerini performans ve güvenilirlik açısından optimize edebilmelerini sağlar.\n",
      "LangChain Kullanımın Faydaları\n",
      "Azaltılmış Geliştirme Süresi:\n",
      "Önceden oluşturulmuş bileşenler ve standartlaştırılmış arayüzler, sıfırdan büyük dil modeli uygulamaları oluşturmaya kıyasla geliştirme süresini önemli ölçüde azaltır.\n",
      "Modüler Tasarım:\n",
      "LangChain, geliştiricilerin dil modeli sağlayıcılarını kolayca değiştirmelerine veya tüm uygulamayı etkilemeden iş akışının belirli bölümlerini değiştirmelerine olanak tanıyan modülerliği teşvik eder.\n",
      "Gelişmiş Performans:\n",
      "LangChain, iletişimi kolaylaştırarak ve olası hataları ele alarak büyük dil modeli etkileşimlerini optimize eder. Daha hızlı yanıt sürelerine ve dil modeli kaynaklarının daha verimli kullanımına yol açabilir.\n",
      "Gelişmiş Yetenekler:\n",
      "LangChain, temel dil modeli etkileşiminin ötesinde işlevlerin kilidini açar. Bilgi entegrasyonu, vektör veritabanları ve özelleştirilebilir iş akışları, geliştiricilerin daha karmaşık ve bilgilendirici uygulamalar oluşturmasını sağlar.\n",
      "Aracılık İşlevselliği:\n",
      "LangChain, çeşitli dil modeli aramalarını, veri alma prosedürlerini ve işleme aşamalarını birleştirerek karmaşık görevleri yürütme yeteneğine sahip kendi kendini yöneten varlıklar olan “ajanlar” kavramını sunar. Ajanlar genişletilebilir ve özelleştirilebilirdir ve geliştiricilere karmaşık yapay zeka odaklı iş akışları tasarlama özgürlüğü sunar. Geliştiriciler, ajanları kullanarak çeşitli veri kaynaklarıyla akıllıca etkileşim kuran karmaşık süreçleri otomatikleştiren ve çok adımlı faaliyetleri gerçekleştiren dinamik uygulamalar oluşturabilir. Bu özellik sayesinde daha akıllı çözümler mümkün hale gelir ve uygulamaların kullanışlılığı ve verimliliği artar.\n",
      "LangChain Kullanımı\n",
      "LangChain’i indirebilmek için:\n",
      "pip install langchain\n",
      "LangChain’i içe aktarmak ve OpenAI gibi bir model oluşturmak için:\n",
      "from langchain import OpenAI\n",
      "ai = OpenAI()\n",
      "Modeli özelleştirmek için:\n",
      "ai = OpenAI(\n",
      "model_name=\"code-davinci-002\", # Codex model\n",
      "temperature=0.7,               # Higher values mean more random\n",
      "max_tokens=100,               # Maximum output length\n",
      "frequency_penalty=0,          # Penalize repetitive text\n",
      "presence_penalty=0            # Penalize off-prompt wandering\n",
      ")\n",
      "Sıcaklık parametresi (temperature), oluşturulan içerikteki yaratıcılık ve tutarlılık arasındaki dengeyi kontrol etmek için kullanılır.\n",
      "Düşük sıcaklık (0,0–0,4): Gerçekçi ve kesin yanıtlar için.\n",
      "Orta sıcaklık (0,5–0,7): Genel içerik oluşturma, yaratıcılık ve tutarlılığı dengeleme için.\n",
      "Daha yüksek sıcaklık (0,8–1,0): Yenilikçi ve beklenmedik çıktılar üretir, beyin fırtınası kampanyaları, sosyal medya içeriği ve bir konu hakkında yeni bakış açıları için harikadır.\n",
      "Metin Üretimi\n",
      "import langchain\n",
      "ai = langchain.OpenAI()\n",
      "def generate_post(title, author):\n",
      "prompt = f\"Generate a blog post on '{title}' by {author}:\"\n",
      "return ai.generate(prompt)print(generate_post(\"Using LangChain\", \"Defne Buse Celik\"))image to grayscale\n",
      "Yalnızca bir başlık ve yazar verildiğinde tüm bir blog yazısı oluşturmak için Codex’i kullanır. Sonuç bir insan tarafından yazılmış gibi tutarlı bir şekilde okunan, çoklu paragraflı sentetik metindir. Bu üreteci, istenen uzunluk, ton, dahil edilecek anahtar kelimeler, gönderi kalitesini karşılayana kadar döngüsel oluşturma, harici bir bilgi tabanı kullanarak gerçekleri doğrulama, taslakların yinelemeli olarak iyileştirilmesine izin verme gibi daha fazla girdi parametresi ekleyerek geliştirebiliriz.\n",
      "Arama ve Soru Cevaplama\n",
      "import langchain\n",
      "codebase = load_codebase()\n",
      "def semantic_search(query):  results = []  for func in codebase:    doc = func.__doc__\n",
      "prompt = f\"Query: {query}\\nDoc: {doc}\\nRelevance:\"\n",
      "score = ai.generate(prompt)\n",
      "results.append({\n",
      "\"function\": func,\n",
      "\"relevance\": score\n",
      "})\n",
      "return sorted(results, key=lambda x: x[\"relevance\"], reverse=True)[:5]print(semantic_search(\"Convert \"))\n",
      "Sorgu ile her fonksiyonun doküman dizisi arasındaki anlamsal alaka düzeyini değerlendirmek için Codex’i kullanır. En alakalı ilk 5 fonksiyonu döndürür. Önbellek dekoratörü, sonuçları yeniden kullanarak pahalı model sorgularını en aza indirir. Diğer arama ve QA uygulamaları arasında belge alma motorları, akıllı ürün önericisi, müşteri desteği için sohbet robotları, bilgi tabanları ve uzman sistem bulunur. Büyük dil modelleri birçok arama ve soru cevaplama alanı için akıl yürütme yetenekleri sağlar.\n",
      "Veri İşleme ve Analizi\n",
      "resumes_db = []\n",
      "ai = OpenAI()@ai.cache\n",
      "def extract_resume(resume_text)\n",
      "name_prompt = f\"Extract the candidate name from this resume:\\n{resume_text}\\nName:\"\n",
      "email_prompt = f\"Extract the candidate email from this resume:\\n{resume_text}\\nEmail:\"\n",
      "email = ai.generate(email_prompt)\n",
      "return {\n",
      "\"name\": name,\n",
      "\"email\": email\n",
      "}\n",
      "for path in resume_paths:\n",
      "text = open(path).read()\n",
      "data = extract_resume(text)\n",
      "resumes_db.append(data)print(resumes_db)\n",
      "Özgeçmiş metnini ayrıştırmak ve yapılandırılmış alanları çıkarmak için Codex’i kullanır. Diğer veri işleme yetenekleri arasında belgeleri kategorilere ayırma, metinden varlıkları çıkarma, hakkında soruları yanıtlama, uzun raporları özetleme yer alır.\n",
      "Akıllı Ajanlar\n",
      "LangChain, Codex gibi modeller kullanarak sanal asistanlar, sohbet robotları ve müşteri hizmetleri temsilcileri oluşturmayı mümkün kılar.\n",
      "histories = {} # Track conversation history\n",
      "ai = OpenAI()  def converse(message, history=[]):\n",
      "history.append(message)    prompt = f\"The conversation history is:\\n\\n{history}\\n\\nHuman: {message}\\nAgent:\"  response = ai.generate(prompt)  history.append(response)  return responsewhile True:  input = get_user_message()\n",
      "response = converse(input, histories[user_id])  print(response)  histories[user_id] = histories[user_id]\n",
      "Codex’ten daha alakalı yanıtlar üretmek için konuşma geçmişi bağlamı kullanılır. Dil modelleri, istekleri anlayan ve doğal konuşmalar yapan asistanlar sağlar.\n",
      "Performans İçin Önbelleğe Alma\n",
      "Büyük dil modellerini sorgulamak zaman alıcı ve yoğun kaynak kullanımına sebep olabilir. LangChain, model çıktılarını kalıcı hale getirmek ve bunların yeniden hesaplanmasını önlemek için çıktıları yerleşik önbelleğe alma sağlar.\n",
      "@ai.cache\n",
      "def generate_summary(text):\n",
      "return ai.generate(\"Summarize this: \" + text)\n",
      "print(generate_summary(\"Some text\"))\n",
      "print(generate_summary(\"Some text\"))\n",
      "Bu, tekrarlanan çağrılarda pahalı model sorgularını atlayarak zamandan ve paradan tasarruf sağlar. Bazı yapılandırılabilir önbelleğe alma parametreleri; önbellek depolama seçeneği — Yerel dosya, Redis veya SQLite, özel önbellek anahtarları ve manuel önbelleğe alma, önbellek boyutu sınırları ve LRU çıkarma, zamana dayalı önbellek süresi sonu, geliştirme sırasında önbelleği yok sayma. Önbelleğe alma, dil modelleri uygulamalarında performansı ve maliyetleri optimize etmeye yardımcı olur.\n",
      "Model Karşılaştırması\n",
      "from langchain.benchmark import Benchmark\n",
      "benchmark = Benchmark(\n",
      "openai=OpenAI(),\n",
      "parti=Parti(),\n",
      "jurassic=Jurassic()\n",
      ")outputs = benchmark.run(\"What is LangChain?\", n=1)print(outputs)\n",
      "İstemi 3 modelin hepsinde çalıştırır ve yanıtlarını bir sözlükte döndürür. Daha sonra; çıktı doğruluğu ve semantiği, gereken belirteç sayısı, API çağrısı başına maliyet, genel sonuç kalitesi gibi farklılıkları programatik olarak analiz edebilebilir. Karşılaştırmalı değerlendirme, uygulamanın işlevsel ve ekonomik ihtiyaçları için doğru modeli belirlemeye yardımcı olacaktır.\n",
      "LangChain Kullanım Alanları Örnekleri\n",
      "Langchain Kullanım Alanları Örnekleri\n",
      "Sohbet Robotları:\n",
      "Geliştiriciler, kullanıcılarla doğal olarak etkileşim kuran, müşteri desteği sağlayan, soruları yanıtlayan ve işlemleri kolaylaştıran akıllı sohbet robotları oluşturabilir.\n",
      "Üretken Soru-Cevaplama (GQA):\n",
      "LangChain, mevcut en güncel verilere dayalı yanıtlar üreten sistemler oluşturmak için kullanılabilir ve yanıtların doğruluğunu ve alakalılığını artırır.\n",
      "Belge Analizi ve Özetleme:\n",
      "Büyük miktarda metni analiz edip özetleyebilen araçların geliştirilmesini sağlayarak önemli içgörülerin çıkarılmasını kolaylaştırır.\n",
      "Kod Analizi:\n",
      "LangChain, olası hatalar veya güvenlik açıkları için kodu analiz etmek ve yazılım kalitesini artırmak için kullanılabilir.\n",
      "Veri Artırma:\n",
      "Makine öğrenimi modellerini eğitmek için yararlı olan mevcut veri kümelerine benzer yeni veriler üretebilir.\n",
      "Sonuç\n",
      "Sonuç olarak, LangChain büyük dil modelleri (LLM) ile entegre uygulamaların geliştirilmesinde önemli bir rol oynar. Sağlam ve esnek mimarisiyle LangChain, geliştiricilerin dil modelleriyle karmaşık zincirler oluşturmasına ve çeşitli görevleri daha verimli bir şekilde çözmesine olanak tanır. Ayrıca, LangSmith entegrasyonuyla bu zincirlerin izlenmesi ve optimize edilmesi daha kolay hale gelir.Almayla Artırılmış Üretim (RAG) gibi teknikler, yalnızca dil modellerine dayanmayan, aynı zamanda harici bilgi kaynaklarına erişen ve daha doğru ve güncel sonuçlar sunan sistemlerin oluşturulmasını sağlar. LangChain tarafından sunulan bu esneklik ve ölçeklenebilirlik, özellikle müşteri desteği, arama motorları, bilgi tabanlı asistanlar ve içerik yönetimi gibi alanlarda önemli avantajlar sağlar. Geliştiriciler, LangChain kullanarak daha dinamik, kullanıcı dostu ve özelleştirilebilir LLM tabanlı çözümler oluşturabilir.\n",
      "Referanslar\n",
      "LangChain Documentation. Available at:\n",
      "https://python.langchain.com\n",
      "Sakamoto, A. (2023, July 24). LangChain Chains: What is LangChain. Kanaries. Available at:\n",
      "https://docs.kanaries.net/articles/langchain-chains-what-is-langchain\n",
      "Langchain\n",
      "Large Language Models\n",
      "Artificial Intelligence\n",
      "Generative Ai Tools\n",
      "Yapay Zeka\n",
      "--\n",
      "--\n",
      "Follow\n",
      "Written by\n",
      "Defne Buse Çelik\n",
      "5 Followers\n",
      "·\n",
      "Writer for\n",
      "Global AI Hub\n",
      "computer engineering student\n",
      "Follow\n",
      "Help\n",
      "Status\n",
      "About\n",
      "Careers\n",
      "Press\n",
      "Blog\n",
      "Privacy\n",
      "Terms\n",
      "Text to speech\n",
      "Teams\n"
     ]
    }
   ],
   "source": [
    "# Let's try one out\n",
    "\n",
    "ed = Website(\"https://medium.com/global-ai-hub/langchain-temelleri-10963adefccd\")\n",
    "print(ed.title)\n",
    "print(ed.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a478a0c-2c53-48ff-869c-4d08199931e1",
   "metadata": {},
   "source": [
    "## Types of prompts\n",
    "\n",
    "\n",
    "\n",
    "**A system prompt** that tells them what task they are performing and what tone they should use\n",
    "\n",
    "**A user prompt** -- the conversation starter that they should reply to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abdb8417-c5dc-44bc-9bee-2e059d162699",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0275b1b-7cfe-4f9d-abfa-7650d378da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"The contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea211b5f-28e1-4a86-8e52-c0b7677cadcc",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "The API from OpenAI expects to receive messages in a particular structure.\n",
    "Many of the other APIs share this structure:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message goes here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user message goes here\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0134dfa4-8299-48b5-b444-f2a8c3403c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "905b9919-aba7-45b5-ae65-81b3d1d78e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e38d41-dfa4-4b20-9c96-c46ea75d9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(\"https://medium.com/global-ai-hub/langchain-temelleri-10963adefccd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d926d59-450e-4609-92ba-2d6f244f1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018853a-445f-41ff-9560-d925d1774b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://medium.com/global-ai-hub/langchain-temelleri-10963adefccd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682eff74-55c4-4d4b-b267-703edbc293c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
